# 术语表

## MPS

NVIDIA MultiProcess Service (MPS)  NVIDIA 多进程服务 (MPS)

## DNN

DNN（深度神经网络）

## SM

streaming multiprocessors (SMs) 流式多处理器

## SLO

服务水平目标

## InSS

Intelligent Scheduling orchestrator for multi-GPU inference servers with spatio-temporal Sharing

用于具有时空共享的多 GPU 推理服务器的智能调度编排器





# 简介

本文提出并实现了 InSS，这是一种在多 GPU 推理服务器上的智能调度编排器，用于通过时空共享实现高吞吐量。我们首先提出一个**分析性能模型，该模型根据干扰和排队延迟来预测任务延迟**。 InSS建立在干扰感知分析模型的基础上，可以**智能地决定模型放置、GPU资源分配和调整批量大小，以最大化系统吞吐量**。大量的原型实验验证了 InSS 的效率。 InSS 可以在满足 SLO 的同时将吞吐量提高高达 86%。

## 挑战

- 同GPU并行模型的干扰
- 推理任务的延迟要求
- 资源分配的碎片化问题

目前大多的GPU资源利用方法没有考虑到碎片化问题，考虑干扰但带来了延迟

- 提出Inss

## 贡献

- 延迟分析模型

  与干扰相关的资源利用率是特定于模型的，并且可以针对每个模型进行独立预测。

- 两阶段智能调度器

  在本地阶段，对于每个 GPU，InSS 利用双延迟深度确定性策略梯度算法 (TD3) 来学习已部署模型的资源分配和批量大小

- 使用十个 DNN 模型和四个 NVIDIA A100 GPU 实现了 InSS 原型

# 背景动机

## 空间共享

- 多进程服务MPS将SM以指定的百分比分配给不同的进程，这种方法仍然涉及剩余 GPU 资源的共享利用。当多个模型并行运行出现干扰。

- 多实例GPU（MIG）是一种物理资源划分方法，在硬件层实现了物理资源的划分，确保了并发进程期间不会发生干扰。完全的资源隔离会导致资源重新分配的时间开销。

MPS延迟更优

## 模型干扰

开发了一个精心设计的模型，用于干扰感知延迟预测。

## 调度开销

在具有 4 个 GPU 和 10 个模型的集群中评估了 InSS 和三种最先进算法的调度开销

## 资源碎片

- 先前的研究缺乏对资源碎片的考虑

# InSS

InSS是GPU集群上的智能调度编排器，它利用GPU分区和批处理来提高吞吐量，同时保证推理任务的SLO要求。

## 架构和工作流

![image-20241029203750998](./inss/image-20241029203750998.png)

- 任务缓冲区
- 监视器收集经验数据并执行预测。两个时间指标：**GPU 执行延迟**和**任务延迟**
- 调度程序利用基于 RL 的两阶段方法来确定系统的模型部署、资源分配和批量大小。最佳批量大小会传递到任务缓冲区，模型放置和资源分配会发送到 GPU 集群。
- GPU集群。任务缓冲区根据调度程序的决策，将任务组织成批次，将数据传输到 GPU 集群进行处理。 GPU集群部署模型，根据调度器确定的配置重新分配GPU资源，接收数据后进行推理计算。

## 总结

- 首先提出一个分析性能模型，该模型根据干扰和排队延迟来预测任务延迟。
- InSS**建立在干扰感知分析模型**的基础上，可以智能地**决定模型放置**、**GPU资源分配**和**调整批量大小**，以**最大化系统吞吐量**。
- 大量的原型实验验证了 InSS 的效率。 InSS 可以在满足 SLO 的同时将吞吐量提高高达 86%。





## 推理服务制定

定义 $GPU_k$ 的计算能力为 $C_k$ ，最大内存限制为 $M_k$ ， $\chi$ 为 ${1,2,...,X}$ 集合

系统提供 $I$ 类模型，定义 $m_i(b_i)$ 为批次大小为 $b_i$ 的模型 $i$ 的内存占用，$SLO_i$ 顾名思义

在时间跨度 $ T $ 内，一组使用模型 $ i $ 的推理任务随机到达在线并请求处理，记为 $ J_i $。DNN 模型 $ i $ 的每个推理任务 $ j \in J_i $ 可以用元组 
$$
\{ a_{i,j}, d_{i,j}, T_{i,j} \}
$$
表示，其中 $ a_{i,j} \in T $ 表示任务 $ j $ 的到达时间，$ d_{i,j} $ 表示任务 $ j $ 的数据大小，$ T_{i,j} $ 表示任务 $ j $ 的任务延迟。

决策变量：
 $y_{i,k} \in \{0, 1\}$，一个二元变量，表示 DNN 模型 $i$ 是否部署在 GPU $k$ 上；
 $\rho_{i,k}$，在 GPU $k$ 上分配给 DNN 模型 $i$ 的计算资源的比例；
 $b_i \in \mathbb{N}^+$，DNN 模型 $i$ 的批量大小。


maximize $ \sum_{\forall i \in I, \forall k \in K} y_{i,k} h_{i,k} $

subject to:
$$
\sum_{i \in I} y_{i,k} \rho_{i,k} \leq 100\%, \quad \forall k \in K \quad (1a) \\
\sum_{i \in I} y_{i,k} m_i(b_i) \leq M_k, \quad \forall k \in K \quad (1b) \\
T_{i,j} \leq SLO_i, \quad \forall i \in I, \forall j \in J_i \quad (1c) \\
\sum_{\forall k \in K} y_{i,k} = 1, \quad \forall i \in I \quad (1d) \\
y_{i,k} \in \{0, 1\}, \quad b_i \in \mathbb{N}^+, \quad \forall i \in I, \forall k \in K \quad (1e)
$$

- 计算资源不超过总量
- 每个GPU的内存容量约束
- 每个任务的任务延迟约束
- 每个任务只部署在一个GPU上

**挑战：**

- NP难问题
- 考虑现实：推理任务到达过程未知
- $T_{i,j}$ 是多个因素相关的未知参数

## 用强化学习学习最优决策

问题转化。为了解决问题 (1)，我们将其重新表述为 MDP。MDP 的特征是元组 $\{ S, A, P, R, \gamma \}$，其中 $S$ 表示状态集合，$A$ 表示动作空间，$P(s'|s, a)$ 表示状态之间转换的概率，$R$ 表示奖励函数，$\gamma \in [0, 1]$ 是折扣因子。在我们的场景中，调度程序被视为代理。问题 (1) 的 MDP 可以设计如下。

i) 状态。在每个决策步骤 $t$，智能体从环境中观察状态，其中包括 DNN 模型 $W_t$ 和 GPU $M_t$ 的信息。对于 DNN 模型，状态包括工作负载参数，如 $\lambda_i$ 的到达率和排队工作负载 $G_i$，即 $W_t = \{\lambda_i, G_i\}_{i \in I}$。这些变量反映了 DNN 模型工作负载的当前状态，并为代理提供了对 GPU 资源需求的洞察。GPU 状态 $k$ 包含前一个动作用于 DNN 模型放置的已用内存 $u_k = \sum_{\forall i \in I} y_{i,k} m_i(b_i)$，即 $M_t = \{u_k\}_{k \in K}$。此信息对于代理跟踪 GPU 内存使用情况、避免内存溢出、防止模型部署失败至关重要。

ii) 行动。给定观察到的状态 $s_t$，智能体在决策步骤 $t$ 确定所有 DNN 模型的动作 $a_t$，其中包括：模型放置、资源分配和批量大小，即 $A_t = \{ y_{i,k}, \rho_{i,k}, b_i \}_{i \in I, k \in K}$。

iii) 奖励。采取行动 $a_t$ 后，智能体会从环境中收到奖励 $r_t$ 来评估行动的质量。与问题（1）的目标一致，奖励函数旨在鼓励导致更高吞吐量的行动。此外，考虑到约束（1b）和（1c），开发了奖励函数来激励代理选择满足约束的动作。让 $v_{i,t}$ 表示在决策步骤 $t$ 违反 SLO 要求的推理任务的百分比。为了模型比较以及易于学习和优化，使用均方根归一化方法来标准化模型吞吐量。将 $\hat{h}_{i,k}$ 表示为 DNN 模型 $i$ 在 GPU $k$ 上的归一化吞吐量。我们将奖励函数制定如下：

$$
r_t = \sum_{i \in I} r_{t,i}，where \ \ r_{i,t} = \sum_{\forall k \in K} y_{i,k} \hat{h}_{i,k} - \omega v_{i,t}
$$

其中 $r_{t,i}$ 表示 DNN 模型 $i$ 的奖励，$\omega$ 是惩罚因素。通过这种方式，参与者会被激励去选择导致更高吞吐量的操作，同时确保满足约束。由于违反约束（1b）会导致模型部署失败，考虑到强化学习的随机性，无法保证约束满足，因此有必要检查内存使用情况，并在发生违反时对操作进行调整。

**传统的状态转移不适合，使用强化学习**

## 训练和可扩展性挑战

在 MPS 资源分配中，以 SM 为粒度操作（如 NVIDIA A100 GPU 的 108 个 SM），同时批量大小为 1 到 32 的整数选项。将变量视为离散结果会导致传统强化学习方法面临指数复杂性和计算负担。转为连续动作空间可提高优化算法的效率，从而加快收敛速度和性能。为此，建议将资源分配和批量大小视为连续变量，形成混合空间。

尽管已有 P-DQN 和 H-PPO 等算法处理混合动作，但其适用性有限，难以扩展，可能导致次优性能。此外，随着 GPU 集群规模扩大，状态和动作空间指数增长，需大量训练数据，增加训练时间并可能导致局部最优。为应对这些挑战，我们提出一种两阶段智能调度程序，采用全局局部分区的方法，提高训练效率和可扩展性。

​	

# 延迟分析模型

InSS设计了一个延迟分析模型作为模拟器，**通过模拟实验进行学习**，无需模型部署，并在真实集群中提供推理服务。干扰感知延迟分析模型可**预测 GPU 执行延迟和任务延迟**，从而解释 DNN 模型和 GPU 中的异构性，以及共置 DNN 模型造成的潜在干扰。

## GPU推理延迟预测

在 GPU $k$ 上使用 DNN 模型 $i$ 的批次的 GPU 执行延迟可以计算为：
$$
t^{in \ f}_{i,k} = t^d_{i,k} + t^c_{i,k} + t^r_{i,k}
$$
其中 $t^d_{i,k}$ 是数据从 CPU 上传到 GPU 的延迟，$t^c_{i,k}$ 是 GPU 计算延迟，$t^r_{i,k}$ 是 GPU 到 CPU 的结果反馈延迟。那么，模型 $i$ 在 GPU $k$ 上的吞吐量可以表述为：
$$
h_{i,k} = \frac{b_i}{t^{in \ f}_{i,k}}
$$


- 数据上传

  数据交互延迟通过PCIe带宽预测

  在 GPU $k$ 上使用 DNN 模型 $i$ 的批次的数据上传延迟和结果反馈延迟可以通过以下方式获得：
  $$
  t_{d, i,k} = \alpha_{d,k} b_i + \beta_{d,k}, \quad t_{r, i,k} = \alpha_{r,k} b_i + \beta_{r,k}
  $$
  其中 $\{\alpha_{d,k}, \beta_{d,k}, \alpha_{r,k}, \beta_{r,k}\}$ 是与可用 PCIe 带宽相关的系数。

- GPU计算

  GPU计算延迟主要关注L2 Cache 和 DRAM，其利用率被用作分配指标

- 结果反馈

  更高的利用率意味着对 GPU 上固定数量资源的竞争加剧，从而导致 GPU 运行时间更长。GPU 运行时延迟可以通过以下方式估计：
  $$
  t^r_{i,k} = o^r_{i,k} \left(1 + \alpha^{\text{cache}}_{i,k} \sum_{\forall i \in I \setminus i} l_{i,k} y_{i,k} + \alpha^{\text{dram}}_{i,k} \sum_{\forall i \in I \setminus i} e_{i,k} y_{i,k}\right)
  $$
  其中 $o^r_{i,k}$ 表示一批 DNN 模型 $i$ 在 GPU $k$ 上单独执行的 GPU 运行时间，$l_{i,k}$ 和 $e_{i,k}$ 分别表示在 GPU 上单独执行的 DNN 模型 $i$ 的 L2 Cache 和 DRAM 利用率，$\alpha^{\text{cache}}_{i,k}$ 和 $\alpha^{\text{dram}}_{i,k}$ 是反映干扰对模型 $i$ GPU 运行时间影响的系数。

批大小对延迟影响大，使用局部加权回归（LWR），GPU运行时间：$ o^r_{i,k} = \text{LWR}(b_i, \rho_{i,k} \cdot C_k) $

最小二乘法拟合$\alpha \ \ \beta$ ，LWR计算运行时间，几秒内就可以获得系数

## 任务延迟预测

- 到达率 符合泊松分布，可以用来拟合

- 任务延迟 忽略传输延迟，重视排队延迟

  令 $g_i(t)$ 表示模型 $i$ 在 $t$（任务到达之前）的排队工作负载。任务 $j$ 之后 DNN 模型 $i$ 的排队工作负载为：
  $$
  w_{i,j} = g_i(a_{i,j}) + d_{i,j}
  $$
  两种情况，任务在任务缓冲区中排队：
  i) 当排队的工作负载足够时（即 $w_{i,j} \geq b_i$），当前任务必须等待较早到达的任务的处理。
  ii) 当排队的工作负载小于批量组装所需的负载时（即 $w_{i,j} < b_i$），当前任务必须等待后续任务的到来，直到排队的工作负载等于批量大小。

  DNN 模型 $i$ 的任务 $j$ 的任务延迟由下式给出：
  $$
  T_{i,j} =
  \begin{cases} 
  \sum_{\forall k \in K} y_{i,k} \text{exec}_{i,k} + \frac{d_{i,j}}{b_i}, & w_{i,j} \geq b_i \\ 
  \max\left(\sum_{\forall k \in K} y_{i,k} \text{exec}_{i,k}, \text{wait}_{i,j}\right) + \sum_{\forall k \in K} y_{i,k} t_{\text{inf}, i,k}, & w_{i,j} < b_i 
  \end{cases}
  $$
  其中 $\text{exec}_{i,k}$ 表示当前处理的 DNN 模型 $i$ 工作负载在 GPU $k$ 上的剩余执行时间，$\text{wait}_{i,j}$ 表示完成批量组装所需的足够工作负载的排队延迟，计算公式为：
  $$
  \text{wait}_{i,j} = \frac{b_i - W_{i,j}}{\lambda_i}.
  $$

- 
